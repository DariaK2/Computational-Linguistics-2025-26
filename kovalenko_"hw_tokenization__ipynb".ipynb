{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DariaK2/Computational-Linguistics-2025-26/blob/main/kovalenko_%22hw_tokenization__ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь, прописала все, включая принт, чтобы отследить результат\n",
        "import re\n",
        "\n",
        "def first(texts):\n",
        "  all_tokens = []\n",
        "  for text in texts:\n",
        "    tokens = re.findall(r'\\w+|[.,!?;:\"()[\\]{}\\\\-]', text)\n",
        "    all_tokens.extend(tokens)\n",
        "  return all_tokens\n",
        "\n",
        "print(first(text))"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "def second(texts):\n",
        "  all_tokens = []\n",
        "  for t in text:\n",
        "    tokens = word_tokenize(t)\n",
        "    all_tokens.extend(tokens)\n",
        "  return all_tokens\n",
        "\n",
        "print(\"\\nNLTK word_tokenize:\", second(text))\n"
      ],
      "metadata": {
        "id": "14BIv33iqrkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def third(texts):\n",
        "  all_tokens = []\n",
        "  for t in texts:\n",
        "    doc = nlp(t)\n",
        "    for token in doc:\n",
        "      all_tokens.append(token.text)  ### сначала написала через extend и получила посимвольное разделение:['T', 'h', 'e', 'q', 'u', 'i', ' ...\n",
        "\n",
        "\n",
        "  return all_tokens\n",
        "\n",
        "\n",
        "print(\"\\nSpaCy tokens:\", third(text))"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvUmk94MhrL8"
      },
      "outputs": [],
      "source": [
        "# Ваш код здесь\n",
        "def fourth(texts):\n",
        "  defs = [first, second, third]\n",
        "  result = {}\n",
        "  for d in defs:\n",
        "    result[d.__name__] = []\n",
        "    for t in texts:\n",
        "      result[d.__name__].append(d([t]))\n",
        "  return result\n",
        "\n",
        "result = fourth(text)\n",
        "for d in result:\n",
        "    print(f\"{d}:\")\n",
        "    for tokens in result[d]:\n",
        "        print(\"  \", \" \".join(tokens))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "Ответ: могу сразу привести примеры возникающих трудностей прямо исходя из задания, 5:30 p.m. превращается при токенизации в бессысленные по отдельности элементы: \"5\", \":\", \"30\"; то же произошло и с денежными данными 1 , 000 . 50 в первой попытке токенизации (def(first)), в остальных случаях можно заметить такие трудности как \"she\", \"'s\" отдельно - то есть глагол или отрицание (например, \"Jane\" и \"'s\", \"dont\" и \"'t\" разбиваются на две условно отдельные единицы, и теряют свой смысл); и третья промлема возникла с словом через дефис - e и mail тоже бессмыслица для модели.\n",
        "Таким образом, простое разделение текста по пробелам и знакам препинания далеко не всегда подходит для задач, стоящих перед NLP-специалистами (из-за богатсва и разнообразия естественного языка в сравнении с искусственыым).\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "нашла вот такой гитхаб https://github.com/openai/tiktoken, который предлагает посчитать с помощью кода ниже (согласно коду 14 токенов)\n",
        "### import tiktoken\n",
        "### enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "### len(enc.encode(\"You shall know a word by the company it keeps\"))\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)\n",
        "Byte Pair Encoding — это способ научить модель разбивать слова на части так, чтобы она могла работать даже с теми словами, которых никогда не видела. Изначально текст разбивается на отдельные символы, и к каждому слову добавляется специальный маркер конца — </w>. Например, слово lowest превращается в ['l', 'o', 'w', 'e', 's', 't', '</w>']. Затем алгоритм смотрит на весь корпус и считает, какие две соседние части встречаются чаще всего. Допустим, пара e и s оказалась самой популярной — ее объединяют в новый токен es. Это повторяется много раз: сначала объединяются буквы, потом уже полученные кусочки, и так далее, пока не получится словарь нужного размера (пр., 30-50 тыс токенов). Когда приходит новое слово, BPE пытается собрать его из уже знакомых частей. Например, lowest может стать ['low', 'est'], а unhappily — ['un', 'happi', 'ly']. Если слово совсем незнакомое, вроде абракадабра, оно разобьется на буквы, но всё равно будет представлено. Суть подхода в отсутствии неизвестных токенов, потому что все можно собрать из кусочков."
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZUS9HTLp58p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}