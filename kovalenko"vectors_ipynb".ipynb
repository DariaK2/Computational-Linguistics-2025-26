{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YgMvYrjW7l7l",
        "yFO8VSjY7rNX",
        "qTOv314oAIUT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DariaK2/Computational-Linguistics-2025-26/blob/main/kovalenko%22vectors_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag-of-Words"
      ],
      "metadata": {
        "id": "YgMvYrjW7l7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "FVZwhv8b3mNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HJDIcKQ3YBb"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel\n",
        "import numpy as np\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Данные для примера\n",
        "documents = [\n",
        "    \"кошка сидит на ковре\",\n",
        "    \"собака бегает по двору\",\n",
        "    \"кошка играет с собакой\",\n",
        "    \"птица летит высоко в небе\",\n",
        "    \"собака и кошка дружат\"\n",
        "]\n",
        "\n",
        "print(\"Исходные документы:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"{i}. {doc}\")"
      ],
      "metadata": {
        "id": "EYJgEpKz3ak2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация\n",
        "tokenized_docs = [doc.lower().split() for doc in documents]\n",
        "print(\"Токенизированные документы:\")\n",
        "pprint(tokenized_docs)"
      ],
      "metadata": {
        "id": "tbEkNS7s3ffp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словаря\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "print(f\"Словарь: {dictionary.token2id}\")"
      ],
      "metadata": {
        "id": "X1MFL-qq3iAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary.token2id"
      ],
      "metadata": {
        "id": "T_vN8hYRs5Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание Bag-of-Words представления\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "print(\"Bag-of-Words векторы (индекс_слова: частота):\")\n",
        "for i, doc_vec in enumerate(bow_corpus, 1):\n",
        "    print(f\"Документ {i}: {doc_vec}\")"
      ],
      "metadata": {
        "id": "-XJQdb1I3nX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "[['кошка', 'сидит', 'на', 'ковре'],\n",
        " ['собака', 'бегает', 'по', 'двору'],\n",
        " ['кошка', 'играет', 'с', 'собакой'],\n",
        " ['птица', 'летит', 'высоко', 'в', 'небе'],\n",
        " ['собака', 'и', 'кошка', 'дружат']]\n",
        " ```"
      ],
      "metadata": {
        "id": "oaFVqYadUtPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Словарь: {'ковре': 0, 'кошка': 1, 'на': 2, 'сидит': 3, 'бегает': 4, 'двору': 5, 'по': 6, 'собака': 7, 'играет': 8, 'с': 9, 'собакой': 10, 'в': 11, 'высоко': 12, 'летит': 13, 'небе': 14, 'птица': 15, 'дружат': 16, 'и': 17}\n",
        "```"
      ],
      "metadata": {
        "id": "XntypDLqUxJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование в плотные векторы для визуализации\n",
        "def bow_to_dense(bow_corpus, dictionary):\n",
        "    dense_vectors = []\n",
        "    for doc in bow_corpus:\n",
        "        dense_vec = [0] * len(dictionary)\n",
        "        for idx, freq in doc:\n",
        "            dense_vec[idx] = freq\n",
        "        dense_vectors.append(dense_vec)\n",
        "    return dense_vectors\n",
        "\n",
        "dense_vectors = bow_to_dense(bow_corpus, dictionary)\n",
        "print(\"Плотные векторы Bag-of-Words:\")\n",
        "print(\"Слова:\", list(dictionary.token2id.keys()))\n",
        "for i, vec in enumerate(dense_vectors, 1):\n",
        "    print(f\"Док {i}: {vec}\")"
      ],
      "metadata": {
        "id": "tfc0g8bP32US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Документ 1: [(0, 1), (1, 1), (2, 1), (3, 1)]\n",
        "Документ 2: [(4, 1), (5, 1), (6, 1), (7, 1)]\n",
        "Документ 3: [(1, 1), (8, 1), (9, 1), (10, 1)]\n",
        "Документ 4: [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]\n",
        "Документ 5: [(1, 1), (7, 1), (16, 1), (17, 1)]\n",
        "```"
      ],
      "metadata": {
        "id": "tBTHEKD5VbbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Визуализация\n",
        "matrix = np.array(dense_vectors)\n",
        "words = list(dictionary.token2id.keys())\n",
        "doc_names = [f\"Doc {i+1}\" for i in range(len(documents))]\n",
        "\n",
        "print(f\"Matrix shape: {matrix.shape}\")\n",
        "\n",
        "plt.figure(figsize=(25, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(matrix,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            xticklabels=words,\n",
        "            yticklabels=doc_names,\n",
        "            cmap='Blues',\n",
        "            cbar_kws={'label': 'Word Frequency'})\n",
        "plt.title('Bag-of-Words Matrix Heatmap')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Documents')\n",
        "plt.xlabel('Words')\n"
      ],
      "metadata": {
        "id": "SvuZ2CT035t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "yFO8VSjY7rNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_model = TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf_model[bow_corpus]\n",
        "\n",
        "print(\"TF-IDF векторы (индекс_слова: tfidf_вес):\")\n",
        "for i, doc_vec in enumerate(tfidf_corpus, 1):\n",
        "    print(f\"Документ {i}: {[(dictionary[idx], round(score, 3)) for idx, score in doc_vec]}\")"
      ],
      "metadata": {
        "id": "E59kDPGY4jeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразуем TF-IDF в плотную матрицу для визуализации\n",
        "def tfidf_to_dense(tfidf_corpus, dictionary):\n",
        "    dense_vectors = []\n",
        "    for doc in tfidf_corpus:\n",
        "        dense_vec = [0.0] * len(dictionary)\n",
        "        for idx, score in doc:\n",
        "            dense_vec[idx] = score\n",
        "        dense_vectors.append(dense_vec)\n",
        "    return dense_vectors\n",
        "\n",
        "tfidf_dense = tfidf_to_dense(tfidf_corpus, dictionary)\n",
        "tfidf_matrix = np.array(tfidf_dense)\n",
        "words = list(dictionary.token2id.keys())\n",
        "doc_names = [f\"Док {i+1}\" for i in range(len(tfidf_corpus))]\n",
        "\n",
        "plt.figure(figsize=(40, 20))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "sns.heatmap(tfidf_matrix,\n",
        "            annot=True,\n",
        "            fmt='.3f',\n",
        "            xticklabels=words,\n",
        "            yticklabels=doc_names,\n",
        "            cmap='YlOrRd',\n",
        "            cbar_kws={'label': 'TF-IDF Вес'})\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "# Получаем исходные TF значения из bow_corpus\n",
        "tf_dense = bow_to_dense(bow_corpus, dictionary)\n",
        "tf_matrix = np.array(tf_dense)\n",
        "\n",
        "# Сравниваем TF и TF-IDF для первого документа\n",
        "doc_idx = 0\n",
        "x_pos = np.arange(len(words))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x_pos - width/2, tf_matrix[doc_idx], width, label='TF', alpha=0.7, color='blue')\n",
        "plt.bar(x_pos + width/2, tfidf_matrix[doc_idx], width, label='TF-IDF', alpha=0.7, color='red')\n",
        "plt.xticks(x_pos, words, rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)"
      ],
      "metadata": {
        "id": "WoUJ8Cyn-GzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. TF (Term Frequency) - Частота термина**\n",
        "```python\n",
        "TF(t,d) = count(t,d) / total_terms(d)\n",
        "```\n",
        "или упрощенно:\n",
        "```python\n",
        "TF(t,d) = count(t,d)  # просто количество вхождений\n",
        "```\n",
        "\n",
        "#### **2. IDF (Inverse Document Frequency) - Обратная частота документа**\n",
        "```python\n",
        "IDF(t) = log( N / (DF(t) + 1) ) + 1\n",
        "```\n",
        "Где:\n",
        "- **N** - общее количество документов\n",
        "- **DF(t)** - количество документов, содержащих слово t\n",
        "- **+1** в знаменателе - сглаживание, чтобы не было \"околонулевых\" значений (smoothing)\n",
        "- **+1** в конце - для избежания нулевых весов\n",
        "\n",
        "#### **3. TF-IDF - итоговый вес**\n",
        "```python\n",
        "TF-IDF(t,d) = TF(t,d) × IDF(t)\n",
        "```\n",
        "\n",
        "**Сравнение разных формул IDF**\n",
        "\n",
        "| Формула | Редкое слово (DF=1) | Частое слово (DF=100) | Сверхчастое (DF=990) |\n",
        "|---------|---------------------|----------------------|---------------------|\n",
        "| `log(N/DF)` | 6.91 | 2.30 | 0.01 |\n",
        "| `log(N/(DF+1))` | 6.90 | 2.29 | 0.009 |\n",
        "| `log(N/(DF+1)) + 1` | **7.90** | **3.29** | **1.009** |\n"
      ],
      "metadata": {
        "id": "4TACHa0o_iky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "qTOv314oAIUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Используем небольшую модель для демонстрации\n",
        "print(\"Загрузка предобученной модели Word2Vec...\")\n",
        "w2v_model = api.load(\"glove-wiki-gigaword-50\")  # Маленькая модель\n",
        "\n",
        "# Функция для получения эмбеддинга документа\n",
        "def get_doc_embedding(tokens, model):\n",
        "    word_vectors = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            word_vectors.append(model[token])\n",
        "        except KeyError:\n",
        "            continue\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Получаем эмбеддинги для документов\n",
        "doc_embeddings = []\n",
        "for i, tokens in enumerate(tokenized_docs):\n",
        "    embedding = get_doc_embedding(tokens, w2v_model)\n",
        "    doc_embeddings.append(embedding)\n",
        "    print(f\"\\nДокумент {i+1}: {documents[i]}\")\n",
        "    print(f\"Эмбеддинг (первые 10 значений): {embedding[:10].round(4)}\")\n",
        "    print(f\"Размер: {embedding.shape}\")\n",
        "\n",
        "# Косинусная схожесть между документами\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(doc_embeddings)\n",
        "print(\"\\nМатрица косинусной схожести:\")\n",
        "print(\"     Д1    Д2    Д3    Д4    Д5\")\n",
        "for i, row in enumerate(similarity_matrix):\n",
        "    print(f\"Д{i+1}  \" + \"  \".join([f\"{x:.3f}\" for x in row]))"
      ],
      "metadata": {
        "id": "3eVRUtVBAO2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Как работает Word2Vec мы разберем на следующей лекции..*"
      ],
      "metadata": {
        "id": "mY6DnHJ5Ar_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Домашка"
      ],
      "metadata": {
        "id": "jzrMnRDOEKho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Скачать [датасет по ссылке](https://raw.githubusercontent.com/tyqiangz/multilingual-sentiment-datasets/refs/heads/main/data/english/test.csv) (просто запустить строку ниже)"
      ],
      "metadata": {
        "id": "2OI0xJWFEMbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tyqiangz/multilingual-sentiment-datasets/refs/heads/main/data/english/test.csv"
      ],
      "metadata": {
        "id": "aHQpBRaRAUNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Открыть csv и сохранить текст в переменную (макс. балл == 1)"
      ],
      "metadata": {
        "id": "wK5JMYLfEVlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('test.csv')\n",
        "\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "print(f\"Всего текстов: {len(texts)}\")\n",
        "print(\"Пример:\", texts[0])"
      ],
      "metadata": {
        "id": "096biNetEU5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Извлечь столбец с текстами (макс. балл == 1)"
      ],
      "metadata": {
        "id": "ziGX47h2E8OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_column = df['text']\n",
        "print(text_column.head())"
      ],
      "metadata": {
        "id": "k2hiqxfiFBwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Построить Bag-of-Words (макс. балл == 3)"
      ],
      "metadata": {
        "id": "nhxbgwC-FCGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Задача 1.1: Создайте словарь и BoW представление (макс. балл == 2)\n",
        "# - Приведите тексты к нижнему регистру\n",
        "# - Удалите знаки препинания\n",
        "# - Постройте словарь\n",
        "# - Преобразуйте документы в BoW векторы\n",
        "\n",
        "import string\n",
        "from gensim import corpora\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text.split()\n",
        "\n",
        "tokenized_texts = [preprocess(t) for t in texts]\n",
        "dictionary = corpora.Dictionary(tokenized_texts)\n",
        "print(f\"Всего уникальных слов: {len(dictionary)}\")\n",
        "bow_corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_texts]\n",
        "\n",
        "# Задача 1.2: Визуализируйте результаты (макс. балл == 1)\n",
        "# - Создайте таблицу частот слов\n",
        "# - Постройте heatmap матрицы BoW\n",
        "# - Найдите самые частые слова в корпусе\n",
        "\n",
        "def bow_to_dense(bow_corpus, dictionary):\n",
        "    dense = []\n",
        "    for doc in bow_corpus:\n",
        "        vec = [0] * len(dictionary)\n",
        "        for word_id, freq in doc:\n",
        "            vec[word_id] = freq\n",
        "        dense.append(vec)\n",
        "    return dense\n",
        "\n",
        "bow_dense = bow_to_dense(bow_corpus, dictionary)\n",
        "bow_matrix = np.array(bow_dense)\n",
        "\n",
        "word_freq = np.sum(bow_matrix, axis=0)\n",
        "freq_df = pd.DataFrame({\n",
        "    'word': [dictionary[i] for i in range(len(dictionary))],\n",
        "    'frequency': word_freq\n",
        "}).sort_values('frequency', ascending=False)\n",
        "\n",
        "print(\"Топ-10 слов:\")\n",
        "print(freq_df.head(10))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "top_words = freq_df.head(20)['word'].tolist()\n",
        "top_indices = [dictionary.token2id[w] for w in top_words]\n",
        "bow_small = bow_matrix[:, top_indices]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(bow_small[:50],  # только первые 50 документов\n",
        "            xticklabels=top_words,\n",
        "            cmap='Blues',\n",
        "            annot=False)\n",
        "plt.title('BoW Heatmap (топ-20 слов, первые 50 доков)')\n",
        "plt.xlabel('Слова')\n",
        "plt.ylabel('Документы')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bfhUa-NuFFv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Построить TF-IDF (макс. балл == 5)"
      ],
      "metadata": {
        "id": "qBFR3AL4FI33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Задача 2.1: Примените TF-IDF к BoW представлению (макс. балл == 2)\n",
        "# - Используйте TfidfModel из gensim\n",
        "# - Получите TF-IDF векторы для каждого документа\n",
        "\n",
        "from gensim.models import TfidfModel\n",
        "\n",
        "tfidf_model = TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf_model[bow_corpus]\n",
        "\n",
        "def tfidf_to_dense(tfidf_corpus, dictionary):\n",
        "    dense = []\n",
        "    for doc in tfidf_corpus:\n",
        "        vec = [0.0] * len(dictionary)\n",
        "        for idx, score in doc:\n",
        "            vec[idx] = score\n",
        "        dense.append(vec)\n",
        "    return dense\n",
        "\n",
        "tfidf_dense = tfidf_to_dense(tfidf_corpus, dictionary)\n",
        "tfidf_matrix = np.array(tfidf_dense)\n",
        "\n",
        "# Задача 2.2: Проанализируйте веса TF-IDF (макс. балл == 3)\n",
        "# - Для каждого слова вычислите: (макс. балл == 2)\n",
        "#   * TF (term frequency) в каждом документе\n",
        "#   * DF (document frequency) во всем корпусе\n",
        "#   * IDF (inverse document frequency)\n",
        "#   * значение TF-IDF\n",
        "# - Сохраните результат в *.сsv (макс. балл == 1)\n",
        "# - Прикрепите *.csv в ваш репозиторий\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def analyze_tfidf_components(bow_corpus, dictionary):\n",
        "  tf_matrix = np.zeros((len(bow_corpus), len(dictionary)))\n",
        "  for doc_idx, doc in enumerate(bow_corpus):\n",
        "    for word_idx, freq in doc:\n",
        "      tf_matrix[doc_idx, word_idx] = freq\n",
        "\n",
        "  df_vector = np.sum(tf_matrix > 0, axis=0)\n",
        "  N = len(bow_corpus)\n",
        "  idf_vector = np.log(N / (df_vector + 1)) + 1\n",
        "\n",
        "  tfidf_manual = tf_matrix * idf_vector\n",
        "\n",
        "  return tf_matrix, df_vector, idf_vector, tfidf_manual\n",
        "\n",
        "tf_matrix, df_vector, idf_vector, tfidf_manual = analyze_tfidf_components(bow_corpus, dictionary)\n",
        "\n",
        "words = ['good', 'bad', 'not', 'movie']\n",
        "words = [w for w in words if w in dictionary.token2id]\n",
        "print(\"Будем анализировать:\", words)\n",
        "\n",
        "rows = []\n",
        "\n",
        "for word in words:\n",
        "    word_id = dictionary.token2id[word]\n",
        "\n",
        "    for doc_idx in range(len(bow_corpus)):\n",
        "        tf = tf_matrix[doc_idx, word_id]        # сколько раз в документе\n",
        "        if tf == 0:\n",
        "            continue  # пропускаем, где слова нет\n",
        "\n",
        "        df = int(df_vector[word_id])            # в скольки доках\n",
        "        idf = round(idf_vector[word_id], 4)\n",
        "        tfidf_val = round(tfidf_manual[doc_idx, word_id], 4)\n",
        "\n",
        "        rows.append({\n",
        "            'word': word,\n",
        "            'document': doc_idx + 1,\n",
        "            'TF': int(tf),\n",
        "            'DF': df,\n",
        "            'IDF': idf,\n",
        "            'TF-IDF': tfidf_val\n",
        "        })\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('tfidf_analysis.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['word', 'document', 'TF', 'DF', 'IDF', 'TF-IDF'])\n",
        "    for row in rows:\n",
        "        writer.writerow([row['word'], row['document'], row['TF'], row['DF'], row['IDF'], row['TF-IDF']])\n",
        "\n",
        "print(\"Сохранено в tfidf_analysis.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "75sjUGaKFLHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Как вычислить компоненты вручную**\n",
        "\n",
        "```python\n",
        "def analyze_tfidf_components(bow_corpus, dictionary):\n",
        "    # Шаг 1: Создаем матрицу TF (term frequency)\n",
        "    tf_matrix = np.zeros((len(bow_corpus), len(dictionary)))\n",
        "    for doc_idx, doc in enumerate(bow_corpus):\n",
        "        for word_idx, freq in doc:\n",
        "            tf_matrix[doc_idx, word_idx] = freq\n",
        "    \n",
        "    # Шаг 2: Вычисляем DF (document frequency)\n",
        "    df_vector = np.sum(tf_matrix > 0, axis=0)  # Количество документов с каждым словом\n",
        "    \n",
        "    # Шаг 3: Вычисляем IDF (inverse document frequency)\n",
        "    N = len(bow_corpus)  # общее количество документов\n",
        "    idf_vector = np.log(N / (df_vector + 1)) + 1\n",
        "    \n",
        "    # Шаг 4: Вычисляем TF-IDF вручную\n",
        "    tfidf_manual = tf_matrix * idf_vector\n",
        "    \n",
        "    return tf_matrix, df_vector, idf_vector, tfidf_manual\n",
        "```"
      ],
      "metadata": {
        "id": "U02KEe1vGQgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Произвести сравните\n",
        "\n",
        "*Эта часть не оценивается, но будет давать доп. баллы на экзамене для тех, кто выполнит её*"
      ],
      "metadata": {
        "id": "WtOZc1HEFc0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Сравните BoW и TF-IDF для конкретных слов\n",
        "# - Выберите 3-4 слова из датасета (выберите слова, характерные для датасета)\n",
        "# - Постройте графики сравнения их весов в BoW и TF-IDF\n",
        "\n",
        "def bow_to_dense(bow_corpus, dictionary):\n",
        "    dense = []\n",
        "    for doc in bow_corpus:\n",
        "        vec = [0] * len(dictionary)\n",
        "        for idx, freq in doc:\n",
        "            vec[idx] = freq\n",
        "        dense.append(vec)\n",
        "    return dense\n",
        "\n",
        "bow_dense = bow_to_dense(bow_corpus, dictionary)\n",
        "bow_matrix = np.array(bow_dense)\n",
        "\n",
        "tfidf_dense = tfidf_to_dense(tfidf_corpus, dictionary)\n",
        "tfidf_matrix = np.array(tfidf_dense)\n",
        "\n",
        "words = ['call', 'free', 'txt', 'win']\n",
        "words = [w for w in words if w in dictionary.token2id]\n",
        "print(\"Графики для слов:\", words)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_bow_tfidf(words, bow_dense, tfidf_dense, dictionary):\n",
        "    n = len(words)\n",
        "\n",
        "    # Правильная сетка\n",
        "    if n <= 2:\n",
        "        fig, axes = plt.subplots(1, n, figsize=(7 * n, 6))\n",
        "        if n == 1:\n",
        "            axes = [axes]\n",
        "        else:\n",
        "            axes = list(axes)\n",
        "    else:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        axes = axes.flatten()[:n]  # обрезаем лишние, если слов < 4\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        word_id = dictionary.token2id[word]\n",
        "\n",
        "        bow_weights = [doc[word_id] for doc in bow_dense]\n",
        "        tfidf_weights = [doc[word_id] for doc in tfidf_dense]\n",
        "        x = range(len(bow_dense))\n",
        "\n",
        "        ax = axes[i]\n",
        "        ax.bar([p - 0.2 for p in x], bow_weights, width=0.4, label='BoW', alpha=0.7, color='skyblue')\n",
        "        ax.bar([p + 0.2 for p in x], tfidf_weights, width=0.4, label='TF-IDF', alpha=0.7, color='salmon')\n",
        "\n",
        "        ax.set_title(f'\"{word}\"', fontsize=13)\n",
        "        ax.set_xlabel('Документы')\n",
        "        ax.set_ylabel('Вес')\n",
        "        ax.legend()\n",
        "\n",
        "        # Подписи — только если документов мало\n",
        "        if len(x) <= 30:\n",
        "            step = max(1, len(x) // 10)\n",
        "            ax.set_xticks(x[::step])\n",
        "            ax.set_xticklabels([f'D{i+1}' for i in x[::step]], rotation=45)\n",
        "        else:\n",
        "            ax.set_xticks([])\n",
        "\n",
        "    plt.suptitle('Сравнение BoW и TF-IDF', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_bow_tfidf(words, bow_dense, tfidf_dense, dictionary)"
      ],
      "metadata": {
        "id": "6gErdrmiFXxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Как построить сравнительный график**\n",
        "```python\n",
        "def compare_bow_tfidf(words, bow_dense, tfidf_dense, dictionary):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    for i, word in enumerate(words):\n",
        "        row, col = i // 2, i % 2\n",
        "        word_id = dictionary.token2id[word]\n",
        "        \n",
        "        # Веса для этого слова во всех документах\n",
        "        bow_weights = [doc[word_id] for doc in bow_dense]\n",
        "        tfidf_weights = [doc[word_id] for doc in tfidf_dense]\n",
        "        \n",
        "        x = range(len(bow_dense))\n",
        "        axes[row, col].bar(x, bow_weights, alpha=0.7, label='BoW', width=0.4)\n",
        "        axes[row, col].bar([p + 0.4 for p in x], tfidf_weights, alpha=0.7, label='TF-IDF', width=0.4)\n",
        "        axes[row, col].set_title(f'Сравнение весов: \"{word}\"')\n",
        "        axes[row, col].legend()\n",
        "        axes[row, col].set_xticks([p + 0.2 for p in x])\n",
        "        axes[row, col].set_xticklabels([f'Док {i+1}' for i in x])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "OuEdfKpIGsjQ"
      }
    }
  ]
}