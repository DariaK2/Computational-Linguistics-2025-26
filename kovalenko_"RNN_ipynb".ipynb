{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "D1QRYFCelTmB",
        "C5TBMM-Wlt2J",
        "xdGK6wCDlzFh",
        "VB0fA8WMl3EC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DariaK2/Computational-Linguistics-2025-26/blob/main/kovalenko_%22RNN_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RNN/LSTM**"
      ],
      "metadata": {
        "id": "-FGiUT4okGYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Цель семинара: получить практический опыт работы с RNN (LSTM) в PyTorch на задачах генерации последовательностей\n",
        "\n",
        "Мы разберем полный пайплайн разработки от препроцессинга данных до обучения и экспериментов с гиперпараметрами\n",
        "\n",
        "Кстати, что мы называем **пайплайном**?"
      ],
      "metadata": {
        "id": "jEVyGGO7kTEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В Google Colab (рекомендуется активировать GPU: `Среда выполнения → Сменить среду выполнения → T4 GPU`)"
      ],
      "metadata": {
        "id": "PO7dhKJVkW60"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YzLBIkqkCA8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import reuters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ЧАСТЬ 1: ДЕМО**\n",
        "*   **Задача модели:** предсказание случайного временного ряда (синусоида)\n",
        "*   **Цель демонстрации:** показать полный и минимальный рабочий пайплайн RNN на задаче, связанной с обработкой данных, передающих изменения во времени\n",
        "*   **План:**\n",
        "    1.  Подготовка последовательностей (sequences)\n",
        "    2.  Архитектура `nn.LSTM`\n",
        "    3.  Обучение модели\n",
        "    4.  Получение метрик оценки\n",
        "    5.  Визуализация и интерпретация результата"
      ],
      "metadata": {
        "id": "ei07Sou1k1zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. ГЕНЕРАЦИЯ И ПОДГОТОВКА ДАННЫХ"
      ],
      "metadata": {
        "id": "D1QRYFCelTmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sine_wave(seq_length=1000, periods=5):\n",
        "    \"\"\"Генерация синусоиды.\"\"\"\n",
        "    x = np.linspace(0, periods * 2 * np.pi, seq_length)\n",
        "    y = np.sin(x)\n",
        "    return y\n",
        "\n",
        "# Генерация данных\n",
        "data = generate_sine_wave()\n",
        "print(f\"Длина временного ряда: {len(data)}\")"
      ],
      "metadata": {
        "id": "6GxGKHcWlWpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры последовательностей\n",
        "SEQ_LEN = 50  # Длина входной последовательности для предсказания\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "dCAM83aVlZfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_len):\n",
        "    \"\"\"Создание пар (входная последовательность, целевое значение).\"\"\"\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - seq_len - 1):\n",
        "        seq = data[i:i + seq_len]\n",
        "        target = data[i + seq_len]\n",
        "        sequences.append(seq)\n",
        "        targets.append(target)\n",
        "    return np.array(sequences), np.array(targets)"
      ],
      "metadata": {
        "id": "ywFp57zSlbiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание последовательностей\n",
        "X, y = create_sequences(data, SEQ_LEN)\n",
        "print(f\"Форма X (последовательности): {X.shape}\")\n",
        "print(f\"Форма y (цели): {y.shape}\")"
      ],
      "metadata": {
        "id": "G80slfBHldUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование в тензоры PyTorch и добавление размерности для признаков\n",
        "X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)  # [примеры, SEQ_LEN, 1]\n",
        "y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)  # [примеры, 1]\n",
        "print(f\"Форма X (тензор): {X.shape}\")\n",
        "print(f\"Форма y (тензор): {y.shape}\")"
      ],
      "metadata": {
        "id": "lRKjxveFle_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение на обучающую и тестовую выборки\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]"
      ],
      "metadata": {
        "id": "PSwQjR9slg08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 2. ОПРЕДЕЛЕНИЕ МОДЕЛИ LSTM\n",
        "class SineLSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=50, num_layers=2, output_size=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True  # формат [batch, seq_len, features]\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Инициализация скрытого состояния и состояния ячейки\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "        # Прямой проход через LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Берем только последний выход последовательности для предсказания\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "aRIvsHbplmEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация модели\n",
        "model = SineLSTM()\n",
        "print(f\"Модель:\\n{model}\")"
      ],
      "metadata": {
        "id": "j8zmPafRlntO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. ОБУЧЕНИЕ МОДЕЛИ"
      ],
      "metadata": {
        "id": "C5TBMM-Wlt2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гиперпараметры\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# Функция потерь и оптимизатор\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Тренировочный цикл\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    # Прямой проход\n",
        "    predictions = model(X_train)\n",
        "    loss = criterion(predictions, y_train)\n",
        "\n",
        "    # Обратное распространение\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    # Оценка на тестовой выборке\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_predictions = model(X_test)\n",
        "        test_loss = criterion(test_predictions, y_test)\n",
        "        test_losses.append(test_loss.item())\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f'Эпоха [{epoch+1}/{EPOCHS}], '\n",
        "              f'Ошибка обучения: {loss.item():.6f}, '\n",
        "              f'Ошибка теста: {test_loss.item():.6f}')"
      ],
      "metadata": {
        "id": "HztyHuLQltR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ"
      ],
      "metadata": {
        "id": "xdGK6wCDlzFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# График 1: Оригинальные данные и предсказания\n",
        "axes[0, 0].plot(data, label='Оригинал', alpha=0.7)\n",
        "axes[0, 0].set_title('Полный временной ряд')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# График 2: Предсказания на тестовой выборке\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_predictions = model(X).numpy()\n",
        "\n",
        "# Создание массива для отображения предсказаний\n",
        "pred_series = np.full_like(data, np.nan)\n",
        "pred_series[SEQ_LEN:SEQ_LEN + len(all_predictions)] = all_predictions.squeeze()\n",
        "\n",
        "axes[0, 1].plot(data, label='Оригинал', alpha=0.7)\n",
        "axes[0, 1].plot(pred_series, label='Предсказания', alpha=0.7, linewidth=2)\n",
        "axes[0, 1].axvline(x=split_idx, color='r', linestyle='--', label='Разделение train/test')\n",
        "axes[0, 1].set_title('Предсказания модели')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# График 3: Ошибки обучения и теста\n",
        "axes[1, 0].plot(train_losses, label='Ошибка обучения')\n",
        "axes[1, 0].plot(test_losses, label='Ошибка теста')\n",
        "axes[1, 0].set_xlabel('Эпоха')\n",
        "axes[1, 0].set_ylabel('MSE')\n",
        "axes[1, 0].set_title('Кривая обучения')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "# График 4: Пример одного предсказания\n",
        "sample_idx = split_idx + 50\n",
        "axes[1, 1].plot(range(SEQ_LEN), X[sample_idx].squeeze().numpy(),\n",
        "                'bo-', label='Входная последовательность')\n",
        "axes[1, 1].axhline(y=y[sample_idx].item(), color='r', linestyle='--',\n",
        "                   label='Истинное значение')\n",
        "axes[1, 1].axhline(y=all_predictions[sample_idx].item(), color='g',\n",
        "                   linestyle='--', label='Предсказание')\n",
        "axes[1, 1].set_xlabel('Шаг времени')\n",
        "axes[1, 1].set_ylabel('Значение')\n",
        "axes[1, 1].set_title('Пример предсказания (один шаг)')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jJ5OrxZckbU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. ИНТЕРПРЕТАЦИЯ РЕЗУЛЬТАТОВ\n",
        "1. Кривая обучения должна снижаться на обеих выборках\n",
        "2. Предсказания должны следовать за оригинальным рядом\n",
        "3. Разрыв между train/test loss указывает на переобучение\n",
        "4. Модель учится предсказывать следующий шаг синусоиды"
      ],
      "metadata": {
        "id": "VB0fA8WMl3EC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take aways:\n",
        "\n",
        "1. Подготовка последовательностей: как из временного ряда создаются пары (окно истории -> целевое значение)?\n",
        "\n",
        "2. Архитектура LSTM: batch_first=True, почему берем out[:, -1, :]\n",
        "\n",
        "3. Цикл обучения: разделение на model.train() и model.eval(), логика .zero_grad(), .backward(), .step()\n",
        "\n",
        "4. Визуализация: как интерпретировать каждый из 4-х графиков?"
      ],
      "metadata": {
        "id": "REWEdXecmd1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "fedJiepGoeL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ПРАКТИКА"
      ],
      "metadata": {
        "id": "3B2CCXsPoy5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ЗАГРУЗКА И ПРЕДВАРИТЕЛЬНЫЙ ПРОСМОТР ДАННЫХ"
      ],
      "metadata": {
        "id": "O_i8NPZSo0Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import reuters"
      ],
      "metadata": {
        "id": "wdNOsuXPCn-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/datasets/reuters/load_data"
      ],
      "metadata": {
        "id": "_aaCBL8ZDPLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 1.1: Загрузите датасет Reuters\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data()\n",
        "\n",
        "# TODO 1.2: Изучите структуру данных\n",
        "print(\"Тип x_train:\", type(x_train))\n",
        "print(\"Длина x_train:\", len(x_train))\n",
        "print(\"Первая последовательность (первые 10 индексов):\", x_train[0][:10])\n",
        "print(\"Длина первой последовательности:\", len(x_train[0]))"
      ],
      "metadata": {
        "id": "rxIa_GD3o5BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. ПОДГОТОВКА СЛОВАРЯ И ДЕКОДИРОВАНИЕ"
      ],
      "metadata": {
        "id": "TCZ18FmTo7u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2.1: Загрузите словарь слов\n",
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}"
      ],
      "metadata": {
        "id": "XOTIh7eAGJ_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2.2: Реализуйте функцию декодирования последовательности\n",
        "def decode_sequence(sequence):\n",
        "    return ' '.join([reverse_word_index.get(i-3, '?') for i in sequence])\n",
        "\n",
        "first_article = decode_sequence(x_train[0])\n",
        "print(\"\\nПервая статья (первые 200 символов):\", first_article[:200])"
      ],
      "metadata": {
        "id": "BpQ6SqWGo9ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ПОДГОТОВКА ДАННЫХ ДЛЯ ГЕНЕРАЦИИ ТЕКСТА"
      ],
      "metadata": {
        "id": "fOuxu5N8o_k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 3.1: Объедините первые 1000 статей в один текст\n",
        "max_articles = 1000\n",
        "max_articles_idx = x_train[:max_articles]\n",
        "max_articles_texts = [decode_sequence(seq) for seq in max_articles_idx]\n",
        "texts = ' '.join(max_articles_texts)\n",
        "\n",
        "print(f\"Общая длина текста (символов): {len(texts)}\")\n",
        "print(\"Пример текста:\", texts[:500])"
      ],
      "metadata": {
        "id": "F1lP_8oJK9Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 3.2: Создайте словари для преобразования символов в индексы и обратно\n",
        "chars = sorted(list(set(texts)))\n",
        "print(chars)"
      ],
      "metadata": {
        "id": "fv83CCBmMNZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(f\"Размер словаря (уникальных символов): {vocab_size}\")"
      ],
      "metadata": {
        "id": "qFh6XHhtMAAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_char"
      ],
      "metadata": {
        "id": "nZMgfbgeAwky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 3.3: Преобразуйте текст в последовательность индексов\n",
        "text_as_int = [char_to_idx[ch] for ch in texts]\n",
        "texts[:10], text_as_int[:10]"
      ],
      "metadata": {
        "id": "-337W_6ZpB8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_int = np.array(text_as_int)\n",
        "\n",
        "text_as_int"
      ],
      "metadata": {
        "id": "0CgNO6kMC3Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. ФОРМИРОВАНИЕ ПРОМПТОВ И ОТВЕТОВ"
      ],
      "metadata": {
        "id": "7hvbErA6pDff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "cNmpMdppD1do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 4.1: Реализуйте функцию create_sequences для создания обучающих примеров\n",
        "def create_sequences(text_indices, seq_len):\n",
        "    input_seq = []\n",
        "    target_seq = []\n",
        "\n",
        "    for i in range(len(text_indices)-seq_len):\n",
        "        current_x_seq = text_indices[i:i+seq_len]\n",
        "        input_seq.append(current_x_seq)\n",
        "\n",
        "        current_y_seq = text_indices[i+seq_len]\n",
        "        target_seq.append(current_y_seq)\n",
        "\n",
        "    return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
        "\n",
        "input_seq, target_seq = create_sequences(text_as_int, seq_length)"
      ],
      "metadata": {
        "id": "fLOSdo5jpFpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq"
      ],
      "metadata": {
        "id": "W4pgFGogGJuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_seq"
      ],
      "metadata": {
        "id": "MID6TP6EGLiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Количество примеров: {len(input_seq)}\")\n",
        "print(f\"Форма input_seq: {input_seq.shape}\")\n",
        "print(f\"Форма target_seq: {target_seq.shape}\")"
      ],
      "metadata": {
        "id": "awleDIoWGRXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 4.2: Создайте DataLoader для пакетной обработки\n",
        "batch_size = 32\n",
        "dataset = torch.utils.data.TensorDataset(input_seq, target_seq)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "e769V-n5D5X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "NzOghrUvIXz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader"
      ],
      "metadata": {
        "id": "JPYI7_2BIZ6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. ОПРЕДЕЛЕНИЕ МОДЕЛИ LSTM"
      ],
      "metadata": {
        "id": "fw81qR_tpHBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size=128, num_layers=2, embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # TODO 5.1: Определите слои модели\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # TODO 5.2: Реализуйте forward pass\n",
        "        # Шаг 1: Примените слой эмбеддинга\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Шаг 2: Прямой проход через LSTM\n",
        "        if hidden is None:\n",
        "            lstm_out, hidden = self.lstm(embedded)\n",
        "        else:\n",
        "            lstm_out, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        # Шаг 3: Возьмите только последний выход LSTM\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "        # Шаг 4: Примените полносвязный слой\n",
        "        output = self.fc(lstm_out)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# TODO 5.3: Инициализируйте модель\n",
        "model = CharLSTM(vocab_size=vocab_size, hidden_size=128, num_layers=2)\n",
        "model = model.to(device)\n",
        "print(f\"\\nМодель:\\n{model}\")"
      ],
      "metadata": {
        "id": "QiJuj5JKpJMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. ОБУЧЕНИЕ МОДЕЛИ"
      ],
      "metadata": {
        "id": "Utv1G8m7pKw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 4\n",
        "LEARNING_RATE = 0.005"
      ],
      "metadata": {
        "id": "dkg6erimNLYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 6.1: Определите функцию потерь и оптимизатор\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "vFuOi_M8NWNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для генерации текста\n",
        "def generate_text(model, seed_text, length=100, temperature=1.0):\n",
        "    \"\"\"Генерация текста на основе начальной последовательности.\"\"\"\n",
        "    model.eval()\n",
        "    generated = seed_text\n",
        "\n",
        "    # Преобразуем seed в индексы, фильтруя символы, которых нет в словаре\n",
        "    seed_indices = []\n",
        "    for ch in seed_text:\n",
        "        if ch in char_to_idx:\n",
        "            seed_indices.append(char_to_idx[ch])\n",
        "        else:\n",
        "            # Если символа нет в словаре, используем первый попавшийся символ\n",
        "            seed_indices.append(0)\n",
        "\n",
        "    if len(seed_indices) == 0:\n",
        "        seed_indices = [0]  # На случай пустого seed\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Создаем начальное hidden state\n",
        "        batch_size = 1\n",
        "        h0 = torch.zeros(model.num_layers, batch_size, model.hidden_size).to(device)\n",
        "        c0 = torch.zeros(model.num_layers, batch_size, model.hidden_size).to(device)\n",
        "        hidden = (h0, c0)\n",
        "\n",
        "        # \"Прогреваем\" модель на seed последовательности\n",
        "        for i in range(len(seed_indices) - 1):\n",
        "            input_tensor = torch.tensor([[seed_indices[i]]], dtype=torch.long).to(device)\n",
        "            _, hidden = model(input_tensor, hidden)\n",
        "\n",
        "        # Начинаем генерацию\n",
        "        current_input = torch.tensor([[seed_indices[-1]]], dtype=torch.long).to(device)\n",
        "\n",
        "        for _ in range(length):\n",
        "            output, hidden = model(current_input, hidden)\n",
        "\n",
        "            # Применяем temperature для управления случайностью\n",
        "            output = output / temperature\n",
        "            probabilities = torch.softmax(output, dim=-1)\n",
        "\n",
        "            # Выбираем следующий символ на основе вероятностей\n",
        "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            # Добавляем символ к сгенерированному тексту\n",
        "            generated += idx_to_char[next_char_idx]\n",
        "\n",
        "            # Обновляем вход для следующей итерации\n",
        "            current_input = torch.tensor([[next_char_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Тренировочный цикл\n",
        "train_losses = []\n",
        "\n",
        "print(\"\\nНачало обучения...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_count = len(dataloader)\n",
        "\n",
        "    for batch_idx, (batch_input, batch_target) in enumerate(dataloader):\n",
        "        # Перемещаем данные на GPU\n",
        "        batch_input = batch_input.to(device)\n",
        "        batch_target = batch_target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Прямой проход\n",
        "        output, _ = model(batch_input)\n",
        "        loss = criterion(output, batch_target)\n",
        "\n",
        "        # Обратное распространение\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Логгирование прогресса каждые 50 батчей\n",
        "        if (batch_idx + 1) % 50 == 0:\n",
        "            avg_batch_loss = loss.item()\n",
        "            print(f'Эпоха [{epoch+1}/{EPOCHS}], Батч [{batch_idx+1}/{batch_count}], Потеря: {avg_batch_loss:.4f}')\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # Генерация текста для мониторинга прогресса\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        # Используем seed текст, который точно есть в нашем словаре\n",
        "        # Преобразуем к нижнему регистру, чтобы гарантировать наличие символов\n",
        "        seed = \"the company said\"\n",
        "        # Альтернативно: можно проверить, какие символы есть в словаре\n",
        "        safe_seed = ''.join([ch for ch in seed if ch in char_to_idx])\n",
        "        if len(safe_seed) > 0:\n",
        "            generated = generate_text(model, safe_seed, length=50, temperature=0.8)\n",
        "            print(f'\\nЭпоха [{epoch+1}/{EPOCHS}], Средняя ошибка: {avg_loss:.4f}')\n",
        "            print(f'Сгенерированный текст: \"{generated}\"')\n",
        "        else:\n",
        "            print(f'\\nЭпоха [{epoch+1}/{EPOCHS}], Средняя ошибка: {avg_loss:.4f}')\n",
        "            print('Не удалось сгенерировать текст: seed содержит неизвестные символы')"
      ],
      "metadata": {
        "id": "mAusHoOkong5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ДОМАШНЕЕ ЗАДАНИЕ (дедлайн: 28 февраля, 23.59)"
      ],
      "metadata": {
        "id": "q1eMr5mpnSRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача:** обучить модель с разными гиперпараметрами\n",
        "\n",
        "1. Набор A: hidden_size=64, num_layers=1, learning_rate=0.01\n",
        "\n",
        "2. Набор B: hidden_size=256, num_layers=2, learning_rate=0.001\n",
        "\n",
        "3. Набор C: любой произвольный набор параметров hidden_size, num_layers, learning_rate, отличный от предложенных\n",
        "\n",
        "Сравните:\n",
        "\n",
        "- Скорость сходимости (график loss)\n",
        "- Качество генерации (осмысленность текста)\n",
        "- Время обучения"
      ],
      "metadata": {
        "id": "7Rj_9ZF6ndZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tensorflow.keras.datasets import reuters"
      ],
      "metadata": {
        "id": "MGrf2_Ga9Cc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "my1MOKQg9kfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data()\n",
        "\n",
        "print(\"Тип x_train:\", type(x_train))\n",
        "print(\"Длина x_train:\", len(x_train))\n",
        "print(\"Первая последовательность (первые 10 индексов):\", x_train[0][:10])\n",
        "print(\"Длина первой последовательности:\", len(x_train[0]))"
      ],
      "metadata": {
        "id": "vIPwmz14-ZC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_sequence(sequence):\n",
        "\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in sequence])\n",
        "\n",
        "first_article = decode_sequence(x_train[0])\n",
        "print(\"\\nПервая статья (первые 200 символов):\", first_article[:200])"
      ],
      "metadata": {
        "id": "G_jTfpFo-j12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_articles = 1000\n",
        "max_articles_idx = x_train[:max_articles]\n",
        "max_articles_texts = [decode_sequence(seq) for seq in max_articles_idx]\n",
        "texts = ' '.join(max_articles_texts)\n",
        "\n",
        "print(f\"Общая длина текста (символов): {len(texts)}\")\n",
        "print(\"Пример текста:\", texts[:500])\n"
      ],
      "metadata": {
        "id": "jc_Ivt5x-woW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(texts)))\n",
        "print(\"Список символов (обрезанный до первых 50):\", chars[:50])\n",
        "\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(f\"Размер словаря (уникальных символов): {vocab_size}\")\n",
        "\n",
        "text_as_int = [char_to_idx[ch] for ch in texts]\n",
        "text_as_int = np.array(text_as_int)\n",
        "\n",
        "print(\"Первые 20 символов текста:\", texts[:20])\n",
        "print(\"Первые 20 индексов:\", text_as_int[:20])\n"
      ],
      "metadata": {
        "id": "94rtdD1I-1Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "\n",
        "def create_sequences(text_indices, seq_len):\n",
        "    input_seq = []\n",
        "    target_seq = []\n",
        "\n",
        "    for i in range(len(text_indices) - seq_len):\n",
        "        current_x_seq = text_indices[i:i + seq_len]\n",
        "        input_seq.append(current_x_seq)\n",
        "\n",
        "        current_y_seq = text_indices[i + seq_len]\n",
        "        target_seq.append(current_y_seq)\n",
        "\n",
        "    input_tensor = torch.tensor(input_seq, dtype=torch.long)\n",
        "    target_tensor = torch.tensor(target_seq, dtype=torch.long)\n",
        "\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "input_seq, target_seq = create_sequences(text_as_int, seq_length)\n",
        "\n",
        "print(f\"Количество примеров: {len(input_seq)}\")\n",
        "print(f\"Форма input_seq: {input_seq.shape}\")\n",
        "print(f\"Форма target_seq: {target_seq.shape}\")\n",
        "\n",
        "batch_size = 32\n",
        "dataset = torch.utils.data.TensorDataset(input_seq, target_seq)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(\"Пример датасета:\", dataset)\n",
        "print(\"Пример даталоадера:\", dataloader)"
      ],
      "metadata": {
        "id": "56v1myUd_HFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size=128, num_layers=2, embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        if hidden is None:\n",
        "            lstm_out, hidden = self.lstm(embedded)\n",
        "        else:\n",
        "            lstm_out, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        output = self.fc(lstm_out)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Используемое устройство:\", device)\n"
      ],
      "metadata": {
        "id": "jZkUAZv_-7Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, length=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    generated = seed_text\n",
        "\n",
        "    seed_indices = []\n",
        "    for ch in seed_text:\n",
        "        if ch in char_to_idx:\n",
        "            seed_indices.append(char_to_idx[ch])\n",
        "        else:\n",
        "            seed_indices.append(0)\n",
        "\n",
        "    if len(seed_indices) == 0:\n",
        "        seed_indices = [0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_size_local = 1\n",
        "        h0 = torch.zeros(model.num_layers, batch_size_local, model.hidden_size).to(device)\n",
        "        c0 = torch.zeros(model.num_layers, batch_size_local, model.hidden_size).to(device)\n",
        "        hidden = (h0, c0)\n",
        "\n",
        "        for i in range(len(seed_indices) - 1):\n",
        "            input_tensor = torch.tensor([[seed_indices[i]]], dtype=torch.long).to(device)\n",
        "            _, hidden = model(input_tensor, hidden)\n",
        "\n",
        "        current_input = torch.tensor([[seed_indices[-1]]], dtype=torch.long).to(device)\n",
        "\n",
        "        for _ in range(length):\n",
        "            output, hidden = model(current_input, hidden)\n",
        "            output = output / temperature\n",
        "            probabilities = torch.softmax(output, dim=-1)\n",
        "\n",
        "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            generated += idx_to_char[next_char_idx]\n",
        "            current_input = torch.tensor([[next_char_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "htZWcP-0AMvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_A = {\n",
        "    \"name\": \"A\",\n",
        "    \"hidden_size\": 64,\n",
        "    \"num_layers\": 1,\n",
        "    \"learning_rate\": 0.01\n",
        "}\n",
        "\n",
        "config_B = {\n",
        "    \"name\": \"B\",\n",
        "    \"hidden_size\": 256,\n",
        "    \"num_layers\": 2,\n",
        "    \"learning_rate\": 0.001\n",
        "}\n",
        "\n",
        "config_C = {\n",
        "    \"name\": \"C\",\n",
        "    \"hidden_size\": 128,\n",
        "    \"num_layers\": 3,\n",
        "    \"learning_rate\": 0.003\n",
        "}\n",
        "\n",
        "EPOCHS = 4\n"
      ],
      "metadata": {
        "id": "Fuhnw_n4ARMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_config(config):\n",
        "    print(f\"\\n=== Обучение конфигурации {config['name']} ===\")\n",
        "    model = CharLSTM(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=config[\"hidden_size\"],\n",
        "        num_layers=config[\"num_layers\"],\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "    train_losses = []\n",
        "\n",
        "    example_batch = next(iter(dataloader))\n",
        "    print(\"Пример размера батча:\", example_batch[0].shape, example_batch[1].shape)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        batch_count = len(dataloader)\n",
        "\n",
        "        for batch_idx, (batch_input, batch_target) in enumerate(dataloader):\n",
        "            batch_input = batch_input.to(device)\n",
        "            batch_target = batch_target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output, _ = model(batch_input)\n",
        "            loss = criterion(output, batch_target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 5000 == 0:\n",
        "                print(f\"[{config['name']}] Эпоха [{epoch+1}/{EPOCHS}], \"\n",
        "                      f\"Батч [{batch_idx+1}/{batch_count}], Потеря: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / batch_count\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"[{config['name']}] Эпоха [{epoch+1}/{EPOCHS}], Средняя потеря: {avg_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            seed = \"the company said\"\n",
        "            safe_seed = ''.join([ch for ch in seed if ch in char_to_idx])\n",
        "            if len(safe_seed) == 0:\n",
        "                safe_seed = \"the \"\n",
        "            generated = generate_text(model, safe_seed, length=80, temperature=0.8)\n",
        "            print(f\"[{config['name']}] Пример сгенерированного текста после эпохи {epoch+1}:\")\n",
        "            print(generated)\n",
        "\n",
        "    return model, train_losses\n"
      ],
      "metadata": {
        "id": "mujsuC3EAXRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "model_A, losses_A = train_with_config(config_A)\n",
        "time_A = time.time() - start_time\n",
        "print(f\"Время обучения для конфигурации A: {time_A:.1f} сек\")\n",
        "\n",
        "start_time = time.time()\n",
        "model_B, losses_B = train_with_config(config_B)\n",
        "time_B = time.time() - start_time\n",
        "print(f\"Время обучения для конфигурации B: {time_B:.1f} сек\")\n",
        "\n",
        "start_time = time.time()\n",
        "model_C, losses_C = train_with_config(config_C)\n",
        "time_C = time.time() - start_time\n",
        "print(f\"Время обучения для конфигурации C: {time_C:.1f} сек\")\n",
        "\n",
        "print(\"Losses A:\", losses_A)\n",
        "print(\"Losses B:\", losses_B)\n",
        "print(\"Losses C:\", losses_C)"
      ],
      "metadata": {
        "id": "-LXBL8ELAsqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "epochs_range = range(1, len(losses_A) + 1)\n",
        "\n",
        "plt.plot(epochs_range, losses_A, label=\"Config A\")\n",
        "plt.plot(epochs_range, losses_B, label=\"Config B\")\n",
        "plt.plot(epochs_range, losses_C, label=\"Config C\")\n",
        "\n",
        "plt.xlabel(\"Эпоха\")\n",
        "plt.ylabel(\"Train loss\")\n",
        "plt.title(\"Кривые сходимости для разных гиперпараметров\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KQ5Wn9mgAxQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ВОПРОСЫ ДЛЯ РЕФЛЕКСИИ**\n",
        "\n",
        "1.  Какие ключевые отличия вы заметили в подготовке данных для числового ряда (синусоида) и текста?\n",
        "\n",
        "2.  Какой опыт работы с PyTorch (например, отладка, подбор параметров) оказался самым полезным?"
      ],
      "metadata": {
        "id": "cn4mIkO5m_eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Для синусоиды данные уже числовые и пайплайн сводится к нарезке окон по времени; для текста пришлось декодировать индексы, собрать корпус и построить словарь символов - и только потом формировать пары(последовательность, следующий символ);"
      ],
      "metadata": {
        "id": "nDJGCILzBg6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. В PyTorch больше всего помогло умение смотреть на .shape тензоров, а также настройка hidden_size, num_layers и learning_rate для разных конфигураций + градиентное отсечение тоже полезно, иначе loss иногда начинал сильно прыгать\n"
      ],
      "metadata": {
        "id": "RTL7DltrCKZA"
      }
    }
  ]
}